# -*- coding: utf-8 -*-
"""higher.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E5XsFArxlbMecBwl7-fCWdDW5vERWz7O
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

ds = pd.read_csv("student-mat.csv")

ds.head()

ds.isnull().values.any()

train_set, test_set=train_test_split(ds, test_size=0.2, random_state=42)

train_set.head()

test_set.head()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('student-mat.csv')  # Replace 'your_dataset.csv' with the actual file path

# Select relevant columns for analysis
data = df[['higher', 'G1', 'G2', 'G3']]

# Convert 'higher' to numeric (if needed)
# You can use LabelEncoder or map 'yes' to 1 and 'no' to 0
data['higher'] = data['higher'].map({'yes': 1, 'no': 0})

# Split the data into features (X) and target variable (y)
X = data[['higher']]
y = data['G1']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Plot the relationship between 'higher' and 'G3'
plt.scatter(X_test, y_test, color='black')
plt.plot(X_test, y_pred, color='blue', linewidth=3)
plt.xlabel('higher')
plt.ylabel('G3')
plt.title('Relationship between higher and G3')
plt.show()

# Print the coefficients of the linear regression model
print('Coefficient:', model.coef_)
print('Intercept:', model.intercept_)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('student-mat.csv')  # Replace 'your_dataset.csv' with the actual file path

# Select relevant columns for analysis
data = df[['higher', 'G1', 'G2', 'G3']]

# One-hot encode 'higher'
data = pd.get_dummies(data, columns=['higher'], drop_first=True)

# Split the data into features (X) and target variable (y)
X = data.drop('G1', axis=1)
y = data['G1']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Plot the relationship between 'higher' and 'G2'
plt.scatter(X_test['higher_yes'], y_test, color='black')  # Assuming 'yes' indicates receiving highert(X_test['higher_yes'], y_pred, color='blue', linewidth=3)
plt.xlabel('higher_yes')
plt.ylabel('G1')
plt.title('Relationship between higher and G1')
plt.show()

# Print the coefficients of the linear regression model
print('Coefficients:', model.coef_)
print('Intercept:', model.intercept_)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt


# Load the dataset
df = pd.read_csv('student-mat.csv')  # Replace 'your_dataset.csv' with the actual file path

# Select relevant columns for analysis
data = df[['higher', 'G1', 'G2', 'G3']]

# One-hot encode 'higher'
data = pd.get_dummies(data, columns=['higher'], drop_first=True)

# Split the data into features (X) and target variable (y)
X = data.drop('G3', axis=1)
y = data['G3']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Plot the relationship between 'higher' and 'G3'
plt.scatter(X_test['higher_yes'], y_test, color='black')  # Assuming 'yes' indicates receiving higher
plt.plot(X_test['higher_yes'], y_pred, color='blue', linewidth=3)
plt.xlabel('higher_yes')
plt.ylabel('G1')
plt.title('Relationship between higher and G1')
plt.show()

# Create a new data point for prediction
# new_data = {'higher_yes': 1, 'G1': 15, 'G2': 16}
# new_data_df = pd.DataFrame([new_data])

X.head()

# Use the trained model to predict G3 for the new data point
# predicted_g3 = model.predict(new_data_df[['higher_yes', 'G1', 'G2']])

# # Display the result
# print("Predicted G3 for a student with higher_yes:", predicted_g3[0])

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('student-mat.csv')  # Replace 'your_dataset.csv' with the actual file path

# Select relevant columns for analysis
data = df[['higher', 'G1', 'G2', 'G3']]

# One-hot encode 'higher'
data = pd.get_dummies(data, columns=['higher'], drop_first=True)

# Split the data into features (X) and target variable (y)
X = data.drop('G1', axis=1)
y = data['G1']


# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Train a linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on the training set
y_test_pred = model.predict(X_test)

print(y_test.values.tolist())
print(y_test_pred)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('student-mat.csv')  # Replace 'your_dataset.csv' with the actual file path

# Handle categorical variables
label_encoder = LabelEncoder()
categorical_columns = ['school', 'sex', 'address', 'famsize', 'Pstatus', 'Mjob', 'Fjob', 'reason', 'guardian',
                        'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic']
for column in categorical_columns:
    df[column] = label_encoder.fit_transform(df[column])

# Split the data into features (X) and target variable (y)
X = df.drop(['G1', 'G2', 'G3'], axis=1)  # Exclude G1, G2, G3 for simplicity
y = df['G1']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize numerical features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Train a RandomForestRegressor model
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

print(y_pred)
print(y_test.values.tolist())

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")

# Visualize actual vs. predicted values
plt.figure(figsize=(10, 6))
sns.scatterplot(x=y_test, y=y_pred, color='blue')
plt.xlabel('Actual Final Grade (G1)')
plt.ylabel('Predicted Final Grade (G1)')
plt.title('Actual vs. Predicted Final Grades')
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('student-mat.csv')  # Replace 'your_dataset.csv' with the actual file path

# Select relevant columns for analysis
data = df[['higher', 'G1', 'G2', 'G3']]

# Calculate the mean of G1, G2, and G3 for each row using .loc
data = data.assign(mean_G=data[['G1', 'G2', 'G3']].mean(axis=1))

# One-hot encode 'higher'
data = pd.get_dummies(data, columns=['higher'], drop_first=True)

# Split the data into features (X) and target variable (y)
X = data[['mean_G', 'higher_yes']]
y = data['G1']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

print(y_test.values.tolist())
print(y_pred)

# Create a DataFrame with the original 'higher' and mean_G columns and predicted/actual G3 values
plot_data = pd.DataFrame({'higher': df.loc[X_test.index, 'higher'], 'Mean G': X_test['mean_G'], 'Actual G1': y_test.values, 'Predicted G1': y_pred})

# Plot the relationship between 'higher' and G3
plt.figure(figsize=(12, 6))
plt.scatter(plot_data['higher'], plot_data['Actual G1'], label='Actual G1', alpha=0.7)
plt.scatter(plot_data['higher'], plot_data['Predicted G1'], label='Predicted G3', marker='x', alpha=0.7)
plt.xlabel('higher')
plt.ylabel('G1')
plt.title('Relationship between higher and G1')
plt.legend()
plt.show()